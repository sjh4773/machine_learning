{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-09T16:17:29.843329Z","iopub.execute_input":"2021-09-09T16:17:29.843659Z","iopub.status.idle":"2021-09-09T16:17:29.851868Z","shell.execute_reply.started":"2021-09-09T16:17:29.843613Z","shell.execute_reply":"2021-09-09T16:17:29.850823Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"from numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:17:31.124472Z","iopub.execute_input":"2021-09-09T16:17:31.124843Z","iopub.status.idle":"2021-09-09T16:17:31.129614Z","shell.execute_reply.started":"2021-09-09T16:17:31.124812Z","shell.execute_reply":"2021-09-09T16:17:31.128601Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"docs = ['Well done!',\n\t\t'Good work',\n\t\t'Great effort',\n\t\t'nice work',\n\t\t'Excellent!',\n\t\t'Weak',\n\t\t'Poor effort!',\n\t\t'not good',\n\t\t'poor work',\n\t\t'Could have done better.']\n\nlabels = array([1,1,1,1,1,0,0,0,0,0])","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:17:34.615978Z","iopub.execute_input":"2021-09-09T16:17:34.616299Z","iopub.status.idle":"2021-09-09T16:17:34.620692Z","shell.execute_reply.started":"2021-09-09T16:17:34.616267Z","shell.execute_reply":"2021-09-09T16:17:34.619873Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"- Tokenizer는 문장으로부터 단어를 토큰화하고 숫자에 대응시키는 딕셔너리를 사용할 수 있도록 합니다.\n- Tokenizer의 인스턴스를 만들면서 num_words 파라미터를 이용해서 단어의 개수를 제한했습니다.\n- 가장 자주 사용되는 num_words-1 개의 단어가 고려됩니다.\n- fit_on_texts() 메서드는 문자 데이터를 입력받아서 리스트의 형태로 변환합니다.\n- tokenizer의 word_index 속성은 단어와 숫자의 키-값 쌍을 포함하는 딕셔너리를 반환합니다.","metadata":{}},{"cell_type":"code","source":"# prepare tokenizer\nt = Tokenizer()\nt.fit_on_texts(docs)\nword_index = t.word_index\nprint(word_index)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:17:39.629689Z","iopub.execute_input":"2021-09-09T16:17:39.630014Z","iopub.status.idle":"2021-09-09T16:17:39.634594Z","shell.execute_reply.started":"2021-09-09T16:17:39.629986Z","shell.execute_reply":"2021-09-09T16:17:39.633696Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"{'work': 1, 'done': 2, 'good': 3, 'effort': 4, 'poor': 5, 'well': 6, 'great': 7, 'nice': 8, 'excellent': 9, 'weak': 10, 'not': 11, 'could': 12, 'have': 13, 'better': 14}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- texts_to_sequences() 메서드는 텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환합니다.\n- sequences를 출력해보면 열 개의 문장이 숫자의 시퀀스로 변환된 것을 확인할 수 있습니다.","metadata":{}},{"cell_type":"code","source":"vocab_size = len(t.word_index) + 1\nprint(vocab_size)\n\n# integer encode the documents\nencoded_docs = t.texts_to_sequences(docs)\nprint(encoded_docs)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:24:47.782859Z","iopub.execute_input":"2021-09-09T16:24:47.783211Z","iopub.status.idle":"2021-09-09T16:24:47.790606Z","shell.execute_reply.started":"2021-09-09T16:24:47.783173Z","shell.execute_reply":"2021-09-09T16:24:47.787808Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"15\n[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Creating Fixed Length Vectors Using Padding","metadata":{}},{"cell_type":"markdown","source":"- pad_sequence() : 전체 훈련 데이터에서 각 샘플의 길이는 서로 다를 수 있습니다. 또는 각 문서 또는 각 문장은 단어의 수가 제각각입니다. 모델의 입력으로 사용하려면 모든 샘플의 길이를 동일하게 맞추어야할 때가 있습니다. 이를 자연어 처리에서는 패딩(padding) 작업이라고 하는데, 보통 숫자 0을 넣어서 길이가 다른 샘플들의 길이를 맞춰줍니다. 케라스에서는 pad_sequence()를 사용합니다. pad_sequence()는 정해준 길이보다 길이가 긴 샘플은 값을 일부 자르고, 정해준 길이보다 길이가 짧은 샘플은 값을 0으로 채웁니다.\n- 첫번째 인자 = 패딩을 진행할 데이터\n- maxlen = 모든 데이터에 대해서 정규화 할 길이\n- padding = 'pre'를 선택하면 앞에 0을 채우고 'post'를 선택하면 뒤에 0을 채움.","metadata":{}},{"cell_type":"code","source":"# pad documents to a max length of 4 words\n\nmax_length = 4\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:17:48.245060Z","iopub.execute_input":"2021-09-09T16:17:48.245391Z","iopub.status.idle":"2021-09-09T16:17:48.251419Z","shell.execute_reply.started":"2021-09-09T16:17:48.245360Z","shell.execute_reply":"2021-09-09T16:17:48.250224Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[[ 6  2  0  0]\n [ 3  1  0  0]\n [ 7  4  0  0]\n [ 8  1  0  0]\n [ 9  0  0  0]\n [10  0  0  0]\n [ 5  4  0  0]\n [11  3  0  0]\n [ 5  1  0  0]\n [12 13  2 14]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Load Glove Word Embedding","metadata":{}},{"cell_type":"markdown","source":"- 워드 임베딩이란 텍스트 내의 단어들을 밀집 벡터(dense vector)로 만드는 것을 말합니다.\n\n- 대부분의 값이 0인 이러한 벡터를 희소 벡터(sparse vector)라고 합니다. 원-핫 벡터는 희소 벡터의 예입니다. 원-핫 벡터는 단어의 수만큼 벡터의 차원을 가지며 단어 간 유사도가 모두 동일하다는 단점이 있습니다. 반면, 희소 벡터와 표기상으로도 의미상으로도 반대인 벡터가 있습니다. 대부분의 값이 실수이고, 상대적으로 저차원인 밀집 벡터(dense vector)입니다. 아래는 밀집 벡터의 예입니다.\n\n- 밀집 벡터는 워드 임베딩 과정을 통해 나온 결과므로 임베딩 벡터(embedding vector)라고도 합니다. \n\n- Ex) [0.1 -1.2 0.8 0.2 1.8] # 상대적으로 저차원이며 실수값을 가짐","metadata":{}},{"cell_type":"markdown","source":"glove.6B.100d.txt 파일은 하나의 줄당 101개의 값을 가지는 리스트를 갖고 있습니다.","metadata":{}},{"cell_type":"markdown","source":"101개의 값 중에서 첫번째 값은 임베딩 벡터가 의미하는 단어를 의미하며, 두번째 값부터 마지막 값은 해당 단어의 임베딩 벡터의 100개의 차원에서의 각 값을 의미합니다. 즉, glove.6B.100d.txt는 수많은 단어에 대해서 100개의 차원을 가지는 임베딩 벡터로 제공하고 있습니다. 위의 출력 결과는 단어 'the'에 대해서 100개의 차원을 가지는 임베딩 벡터와 단어 ','에 대해서 100개의 차원을 가지는 임베딩 벡터를 보여줍니다. ","metadata":{}},{"cell_type":"markdown","source":"형식은 키(key)와 값(value)의 쌍(pair)를 가지는 파이썬의 사전형 구조를 사용합니다.","metadata":{}},{"cell_type":"code","source":"# load the whole embedding into memory\nembeddings_index = dict()\nf = open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index)  )","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:21:44.781428Z","iopub.execute_input":"2021-09-09T16:21:44.781781Z","iopub.status.idle":"2021-09-09T16:22:01.328875Z","shell.execute_reply.started":"2021-09-09T16:21:44.781749Z","shell.execute_reply":"2021-09-09T16:22:01.326349Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Loaded 400000 word vectors.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Creating Embedding Matrix","metadata":{}},{"cell_type":"markdown","source":"- word_index.items()는 key와 value의 쌍을 튜플로 묶은 값을 dict_items 객체로 리턴한다\n- ex) dict_items([('점심', 1), ('나랑', 2), ('먹으러', 3), ('갈래', 4), ('메뉴는', 5), ('햄버거', 6), ('메뉴', 7), ('좋지', 8)])","metadata":{}},{"cell_type":"code","source":"# create a weight matrix for words in trainging docs\nembedding_matrix = zeros((vocab_size, 100))\nfor word, i in t.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:28:16.558942Z","iopub.execute_input":"2021-09-09T16:28:16.559268Z","iopub.status.idle":"2021-09-09T16:28:16.565167Z","shell.execute_reply.started":"2021-09-09T16:28:16.559238Z","shell.execute_reply":"2021-09-09T16:28:16.563995Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Creating Model Using Glove Embedding","metadata":{}},{"cell_type":"markdown","source":"- Embedding() : Embedding()은 단어를 밀집 벡터로 만드는 역할을 합니다. 인공 신경망 용어로는 임베딩 층(embedding layer)을 만드는 역할을 합니다. Embedding()은 정수 인코딩이 된 단어들을 입력을 받아서 임베딩을 수행합니다.\n\n- Embedding()은 (number of samples, input_length)인 2D 정수 텐서를 입력받습니다. 이 때 각 sample은 정수 인코딩이 된 결과로, 정수의 시퀀스입니다. Embedding()은 워드 임베딩 작업을 수행하고 (number of samples, input_length, embedding word dimensionality)인 3D 텐서를 리턴합니다.\n\n- ex) Embedding(7, 2, input_length=5)\n - 7은 단어의 개수. 즉, 단어 집합(vocabulary)의 크기이다.\n - 2는 임베딩한 후의 벡터의 크기이다.\n - 5는 각 입력 시퀀스의 길이. 즉, input_length이다.\n \n- Dense() : 전결합층(fully-conntected layer)을 추가합니다. model.add()를 통해 추가할 수 있습니다.\n- 첫번째 인자 = 출력 뉴런의 수.\n- input_dim = 입력 뉴런의 수. (입력의 차원)\n- activation = 활성화 함수.\n\n\n-  sigmoid : 시그모이드 함수. 이진 분류 문제에서 출력층에 주로 사용되는 활성화 함수.","metadata":{}},{"cell_type":"markdown","source":"사전 훈련된 워드 임베딩을 100차원의 값인 것으로 사용하고 있기 때문에 임베딩 층의 output_dim의 인자값으로 100을 주어야 합니다. 그리고 사전 훈련된 워드 임베딩을 그대로 사용할 것이므로, 별도로 더 이상 훈련을 하지 않는다는 옵션을 줍니다. 이는 trainable=False로 선택할 수 있습니다.","metadata":{}},{"cell_type":"code","source":"# define model\nmodel = Sequential()\ne = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\nmodel.add(e)\nmodel.add(Flatten())\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# summarize the model\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:41:35.022723Z","iopub.execute_input":"2021-09-09T16:41:35.023040Z","iopub.status.idle":"2021-09-09T16:41:35.098746Z","shell.execute_reply.started":"2021-09-09T16:41:35.023012Z","shell.execute_reply":"2021-09-09T16:41:35.097020Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 4, 100)            1500      \n_________________________________________________________________\nflatten (Flatten)            (None, 400)               0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 401       \n=================================================================\nTotal params: 1,901\nTrainable params: 401\nNon-trainable params: 1,500\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"# fit the model\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\n\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","metadata":{"execution":{"iopub.status.busy":"2021-09-09T16:51:42.269501Z","iopub.execute_input":"2021-09-09T16:51:42.269833Z","iopub.status.idle":"2021-09-09T16:51:43.921282Z","shell.execute_reply.started":"2021-09-09T16:51:42.269803Z","shell.execute_reply":"2021-09-09T16:51:43.919099Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Accuracy: 100.000000\n","output_type":"stream"}]}]}